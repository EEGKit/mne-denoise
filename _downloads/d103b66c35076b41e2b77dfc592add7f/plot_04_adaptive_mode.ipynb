{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ZapLine-plus: Paper-Faithful Example (Klug & Kloosterman 2022)\n\nThis example replicates the core demonstrations from the Zapline-plus paper,\nshowing the adaptive cleaning process on synthetic data.\n\nThe paper (Klug & Kloosterman 2022) demonstrates:\n1. Automatic noise frequency detection\n2. Adaptive chunk segmentation based on covariance stationarity\n3. Per-chunk component removal with outlier detection\n4. QA loop for under/over-cleaning adaptation\n\nWe create visualizations similar to the paper figures to demonstrate our\nPython implementation matches the MATLAB reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Sina Esmaeili <sina.esmaeili@umontreal.ca>\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.gridspec import GridSpec\nfrom scipy import signal\n\nfrom mne_denoise.viz.zapline import plot_psd_comparison\nfrom mne_denoise.zapline.adaptive import (\n    check_artifact_presence,\n    find_fine_peak,\n    find_noise_freqs,\n    segment_data,\n)\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paper Parameters\nMatch the paper's recommended settings for comparison.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "PAPER_PARAMS = {\n    \"minfreq\": 17,  # Minimum search frequency (Hz)\n    \"maxfreq\": 99,  # Maximum search frequency (Hz)\n    \"detectionWinsize\": 6,  # Window size for detection (Hz)\n    \"coarseFreqDetectPowerDiff\": 4,  # Threshold (10*log10 units, ~2.5x power)\n    \"noiseCompDetectSigma\": 3.0,  # Initial sigma for outlier detection\n    \"minSigma\": 2.5,  # Minimum sigma for adaptation\n    \"maxSigma\": 4.0,  # Maximum sigma for adaptation (paper says 5, but 4 is safer)\n    \"minChunkLength\": 30,  # Minimum chunk length (seconds)\n    \"segmentLength\": 1,  # Segment length for covariance (seconds)\n    \"freqDetectMultFine\": 2,  # Multiplier for fine detection threshold\n    \"detailedFreqBoundsUpper\": [-0.05, 0.05],  # For \"too weak\" check\n    \"detailedFreqBoundsLower\": [-0.4, 0.1],  # For \"too strong\" check\n    \"maxProportionAboveUpper\": 0.005,\n    \"maxProportionBelowLower\": 0.005,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulate Realistic Non-Stationary Data\nFollowing the paper's approach: line noise that varies in topography and\nstrength over time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def generate_nonstationary_data(\n    sfreq=250,\n    duration=180,  # 3 minutes\n    n_ch=32,\n    line_freq=50.0,\n    seed=42,\n):\n    \"\"\"Generate data with non-stationary line noise.\n    Creates a scenario with:\n    - Chunk 1 (0-60s): Strong 50 Hz, Topography A\n    - Chunk 2 (60-120s): Moderate 50.05 Hz (drift), Topography B\n    - Chunk 3 (120-180s): Weak/absent noise (bursts only)\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    n_times = int(duration * sfreq)\n    times = np.arange(n_times) / sfreq\n\n    # Background brain activity (pink noise-like)\n    # Simulate 1/f spectrum using filtered noise\n    brain = rng.randn(n_ch, n_times)\n    # Low-pass to simulate neural activity spectrum\n    sos = signal.butter(4, 40, btype=\"low\", fs=sfreq, output=\"sos\")\n    brain = signal.sosfiltfilt(sos, brain, axis=1)\n    brain = brain / np.std(brain) * 0.5  # Normalize\n\n    # Define segment boundaries\n    t1 = int(60 * sfreq)\n    t2 = int(120 * sfreq)\n\n    # Topography A: Frontal-like (strong on first channels)\n    topo_a = np.exp(-np.linspace(0, 4, n_ch))\n    topo_a /= np.linalg.norm(topo_a)\n\n    # Topography B: Posterior-like (strong on last channels)\n    topo_b = np.exp(-np.linspace(0, 4, n_ch)[::-1])\n    topo_b /= np.linalg.norm(topo_b)\n\n    # Topography C: Bipolar (alternating)\n    topo_c = np.sin(np.linspace(0, 2 * np.pi, n_ch))\n    topo_c /= np.linalg.norm(topo_c)\n\n    # Initialize noise array\n    noise = np.zeros((n_ch, n_times))\n\n    # Chunk 1: Strong 50 Hz\n    freq1 = line_freq\n    ts1 = np.sin(2 * np.pi * freq1 * times[:t1])\n    # Add harmonics (realistic line noise)\n    ts1 += 0.3 * np.sin(2 * np.pi * 2 * freq1 * times[:t1])\n    ts1 += 0.1 * np.sin(2 * np.pi * 3 * freq1 * times[:t1])\n    noise[:, :t1] = np.outer(topo_a, ts1) * 3.0\n\n    # Chunk 2: Moderate 50.05 Hz (frequency drift)\n    freq2 = line_freq + 0.05\n    ts2 = np.sin(2 * np.pi * freq2 * times[t1:t2])\n    ts2 += 0.3 * np.sin(2 * np.pi * 2 * freq2 * times[t1:t2])\n    noise[:, t1:t2] = np.outer(topo_b, ts2) * 1.5\n\n    # Chunk 3: Bursts only\n    # 10% of time has noise, 90% clean\n    freq3 = line_freq - 0.05\n    ts3 = np.sin(2 * np.pi * freq3 * times[t2:])\n    burst_mask = np.zeros(n_times - t2)\n    n_bursts = 5\n    burst_len = int(sfreq * 2)  # 2s bursts\n    for i in range(n_bursts):\n        start = rng.randint(0, n_times - t2 - burst_len)\n        burst_mask[start : start + burst_len] = 1.0\n    noise[:, t2:] = np.outer(topo_c, ts3 * burst_mask) * 0.5\n\n    data = brain + noise\n\n    return (\n        data,\n        times,\n        {\n            \"t1\": t1,\n            \"t2\": t2,\n            \"freq1\": freq1,\n            \"freq2\": freq2,\n            \"freq3\": freq3,\n            \"topo_a\": topo_a,\n            \"topo_b\": topo_b,\n            \"topo_c\": topo_c,\n        },\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Generating non-stationary synthetic data...\")\nsfreq = 250\ndata, times, sim_info = generate_nonstationary_data(sfreq=sfreq)\nn_ch, n_times = data.shape\nduration = n_times / sfreq\n\nprint(f\"  Shape: {n_ch} channels \u00d7 {n_times} samples ({duration:.0f}s)\")\nprint(\"  Noise structure:\")\nprint(f\"    0-60s:    {sim_info['freq1']} Hz, strong, frontal topography\")\nprint(f\"    60-120s:  {sim_info['freq2']} Hz, moderate, posterior topography\")\nprint(f\"    120-180s: {sim_info['freq3']} Hz, weak bursts, bipolar topography\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Automatic Noise Frequency Detection\nThe paper uses a Welch PSD with Hanning window, then searches for peaks\nthat exceed the \"center power\" (mean of flanks) by > 4 dB.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Step 1: Automatic Frequency Detection ---\")\ndetected_freqs = find_noise_freqs(\n    data,\n    sfreq,\n    fmin=PAPER_PARAMS[\"minfreq\"],\n    fmax=PAPER_PARAMS[\"maxfreq\"],\n    window_length=PAPER_PARAMS[\"detectionWinsize\"],\n    threshold_factor=PAPER_PARAMS[\"coarseFreqDetectPowerDiff\"],\n)\nprint(f\"Detected noise frequencies: {detected_freqs}\")\n\n# Visualize detection\nfig, ax = plt.subplots(figsize=(10, 4))\nf, psd = signal.welch(data, fs=sfreq, nperseg=int(sfreq * 4), axis=-1)\npsd_log = 10 * np.log10(np.clip(psd, 1e-20, None))\nmean_psd = np.mean(psd_log, axis=0)\n\nax.plot(f, mean_psd, \"k-\", lw=1)\nfor freq in detected_freqs:\n    ax.axvline(freq, color=\"r\", ls=\"--\", alpha=0.7, label=f\"Detected: {freq:.1f} Hz\")\nax.set_xlim(40, 70)\nax.set_xlabel(\"Frequency (Hz)\")\nax.set_ylabel(\"Power (10*log10)\")\nax.set_title(\"Automatic Noise Frequency Detection\")\nax.legend()\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Adaptive Data Segmentation\nSegment data based on covariance stationarity (changing noise topography).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Step 2: Adaptive Segmentation ---\")\ntarget_freq = detected_freqs[0] if detected_freqs else 50.0\nsegments = segment_data(\n    data, sfreq, target_freq, min_chunk_len=PAPER_PARAMS[\"minChunkLength\"]\n)\nprint(f\"Number of segments: {len(segments)}\")\nfor i, (start, end) in enumerate(segments):\n    print(\n        f\"  Segment {i + 1}: {start / sfreq:.1f}s - {end / sfreq:.1f}s ({(end - start) / sfreq:.1f}s)\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Per-Segment Processing\nFor each segment:\n1. Fine-tune frequency\n2. Check artifact presence\n3. Run ZapLine with adaptive component selection\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Step 3: Per-Segment Processing ---\")\nsegment_info = []\nfor i, (start, end) in enumerate(segments):\n    chunk = data[:, start:end]\n\n    # Fine-tune frequency\n    fine_freq = find_fine_peak(chunk, sfreq, target_freq, search_width=0.1)\n\n    # Check presence\n    present = check_artifact_presence(chunk, sfreq, fine_freq)\n\n    segment_info.append(\n        {\"start\": start, \"end\": end, \"fine_freq\": fine_freq, \"present\": present}\n    )\n\n    status = \"DETECTED\" if present else \"NOT DETECTED\"\n    print(f\"  Segment {i + 1}: Fine freq = {fine_freq:.3f} Hz, Artifact {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Apply Zapline-plus\nUse the full pipeline with QA loop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Step 4: Apply Zapline-plus ---\")\nfrom mne_denoise.zapline import ZapLine\n\n# Configure adaptive ZapLine\nzl = ZapLine(\n    sfreq=sfreq,\n    line_freq=target_freq,  # Use detected frequency or None\n    adaptive=True,\n    adaptive_params={\n        \"fmin\": PAPER_PARAMS[\"minfreq\"],\n        \"fmax\": PAPER_PARAMS[\"maxfreq\"],\n        \"n_remove_params\": {\n            \"sigma\": PAPER_PARAMS[\"noiseCompDetectSigma\"],\n            \"min_remove\": 1,\n            \"max_prop\": 0.2,\n        },\n        \"qa_params\": {\n            \"max_sigma\": PAPER_PARAMS[\"maxSigma\"],\n            \"min_sigma\": PAPER_PARAMS[\"minSigma\"],\n        },\n    },\n)\n\n# Run Fit+Transform\ndata_clean = zl.fit_transform(data)\nresult = zl.adaptive_results_\n\nprint(f\"Cleaning complete. Components removed: {result['n_removed']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paper-Style Results Visualization\nUse our reusable viz functions for quick overview, then custom plots.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Quick overview using our viz functions\nplot_psd_comparison(data, data_clean, sfreq, line_freq=target_freq, show=True)\n\nfig = plt.figure(figsize=(14, 10))\ngs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n\n# 1. Original vs Cleaned PSD (Full)\nax1 = fig.add_subplot(gs[0, :2])\nf_orig, psd_orig = signal.welch(data, fs=sfreq, nperseg=int(sfreq * 4), axis=-1)\nf_clean, psd_clean = signal.welch(data_clean, fs=sfreq, nperseg=int(sfreq * 4), axis=-1)\n\nmean_orig = np.mean(10 * np.log10(np.clip(psd_orig, 1e-20, None)), axis=0)\nmean_clean = np.mean(10 * np.log10(np.clip(psd_clean, 1e-20, None)), axis=0)\n\nax1.plot(f_orig, mean_orig, \"k-\", lw=1, label=\"Original\")\nax1.plot(f_clean, mean_clean, \"g-\", lw=1.5, label=\"Cleaned\")\nax1.set_xlim(0, 100)\nax1.set_xlabel(\"Frequency (Hz)\")\nax1.set_ylabel(\"Power (dB)\")\nax1.set_title(\"Power Spectral Density: Full Range\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Zoom on Noise Frequency\nax2 = fig.add_subplot(gs[0, 2])\nax2.plot(f_orig, mean_orig, \"k-\", lw=1, label=\"Original\")\nax2.plot(f_clean, mean_clean, \"g-\", lw=1.5, label=\"Cleaned\")\nax2.set_xlim(target_freq - 2, target_freq + 2)\nax2.set_xlabel(\"Frequency (Hz)\")\nax2.set_title(f\"Zoom: {target_freq:.1f} Hz \u00b1 2 Hz\")\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 3. Spectrogram (Original)\nax3 = fig.add_subplot(gs[1, 0])\nf_spec, t_spec, Sxx = signal.spectrogram(\n    data[0], fs=sfreq, nperseg=int(sfreq * 2), noverlap=int(sfreq)\n)\nim = ax3.pcolormesh(\n    t_spec,\n    f_spec,\n    10 * np.log10(Sxx + 1e-20),\n    shading=\"gouraud\",\n    cmap=\"viridis\",\n    vmin=-30,\n    vmax=10,\n)\nax3.set_ylim(45, 55)\nax3.set_xlabel(\"Time (s)\")\nax3.set_ylabel(\"Frequency (Hz)\")\nax3.set_title(\"Original (Ch 0)\")\nplt.colorbar(im, ax=ax3, label=\"dB\")\n\n# 4. Spectrogram (Cleaned)\nax4 = fig.add_subplot(gs[1, 1])\nf_spec, t_spec, Sxx_clean = signal.spectrogram(\n    data_clean[0], fs=sfreq, nperseg=int(sfreq * 2), noverlap=int(sfreq)\n)\nim = ax4.pcolormesh(\n    t_spec,\n    f_spec,\n    10 * np.log10(Sxx_clean + 1e-20),\n    shading=\"gouraud\",\n    cmap=\"viridis\",\n    vmin=-30,\n    vmax=10,\n)\nax4.set_ylim(45, 55)\nax4.set_xlabel(\"Time (s)\")\nax4.set_ylabel(\"Frequency (Hz)\")\nax4.set_title(\"Cleaned (Ch 0)\")\nplt.colorbar(im, ax=ax4, label=\"dB\")\n\n# 5. Spectrogram (Removed)\nax5 = fig.add_subplot(gs[1, 2])\nremoved = data - data_clean\nf_spec, t_spec, Sxx_rem = signal.spectrogram(\n    removed[0], fs=sfreq, nperseg=int(sfreq * 2), noverlap=int(sfreq)\n)\nim = ax5.pcolormesh(\n    t_spec,\n    f_spec,\n    10 * np.log10(Sxx_rem + 1e-20),\n    shading=\"gouraud\",\n    cmap=\"viridis\",\n    vmin=-30,\n    vmax=10,\n)\nax5.set_ylim(45, 55)\nax5.set_xlabel(\"Time (s)\")\nax5.set_ylabel(\"Frequency (Hz)\")\nax5.set_title(\"Removed (Ch 0)\")\nplt.colorbar(im, ax=ax5, label=\"dB\")\n\n# 6. Per-Segment Comparison\nt1 = sim_info[\"t1\"]\nt2 = sim_info[\"t2\"]\n\nax6 = fig.add_subplot(gs[2, 0])\nf1, p1_o = signal.welch(data[:, :t1], fs=sfreq, nperseg=int(sfreq * 2), axis=-1)\nf1, p1_c = signal.welch(data_clean[:, :t1], fs=sfreq, nperseg=int(sfreq * 2), axis=-1)\nax6.plot(f1, np.mean(10 * np.log10(p1_o + 1e-20), axis=0), \"k-\", label=\"Original\")\nax6.plot(f1, np.mean(10 * np.log10(p1_c + 1e-20), axis=0), \"g-\", label=\"Cleaned\")\nax6.set_xlim(45, 55)\nax6.set_title(\"Segment 1: 0-60s (Strong)\")\nax6.set_xlabel(\"Frequency (Hz)\")\nax6.set_ylabel(\"Power (dB)\")\nax6.legend()\nax6.grid(True, alpha=0.3)\n\nax7 = fig.add_subplot(gs[2, 1])\nf2, p2_o = signal.welch(data[:, t1:t2], fs=sfreq, nperseg=int(sfreq * 2), axis=-1)\nf2, p2_c = signal.welch(data_clean[:, t1:t2], fs=sfreq, nperseg=int(sfreq * 2), axis=-1)\nax7.plot(f2, np.mean(10 * np.log10(p2_o + 1e-20), axis=0), \"k-\", label=\"Original\")\nax7.plot(f2, np.mean(10 * np.log10(p2_c + 1e-20), axis=0), \"g-\", label=\"Cleaned\")\nax7.set_xlim(45, 55)\nax7.set_title(\"Segment 2: 60-120s (Moderate)\")\nax7.set_xlabel(\"Frequency (Hz)\")\nax7.legend()\nax7.grid(True, alpha=0.3)\n\nax8 = fig.add_subplot(gs[2, 2])\nf3, p3_o = signal.welch(data[:, t2:], fs=sfreq, nperseg=int(sfreq * 2), axis=-1)\nf3, p3_c = signal.welch(data_clean[:, t2:], fs=sfreq, nperseg=int(sfreq * 2), axis=-1)\nax8.plot(f3, np.mean(10 * np.log10(p3_o + 1e-20), axis=0), \"k-\", label=\"Original\")\nax8.plot(f3, np.mean(10 * np.log10(p3_c + 1e-20), axis=0), \"g-\", label=\"Cleaned\")\nax8.set_xlim(45, 55)\nax8.set_title(\"Segment 3: 120-180s (Weak/Bursts)\")\nax8.set_xlabel(\"Frequency (Hz)\")\nax8.legend()\nax8.grid(True, alpha=0.3)\n\nfig.suptitle(\"Zapline-plus Results: Non-Stationary Line Noise Removal\", fontsize=14)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantitative Evaluation\nCompute metrics similar to the paper.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Quantitative Evaluation ---\")\n\n\ndef compute_noise_ratio(data_orig, data_clean, sfreq, noise_freq, win=0.5):\n    \"\"\"Compute ratio of power at noise freq to surroundings.\"\"\"\n    f, psd_o = signal.welch(data_orig, fs=sfreq, nperseg=int(sfreq * 2), axis=-1)\n    f, psd_c = signal.welch(data_clean, fs=sfreq, nperseg=int(sfreq * 2), axis=-1)\n\n    psd_o = np.mean(psd_o, axis=0)\n    psd_c = np.mean(psd_c, axis=0)\n\n    # Noise band\n    noise_mask = (f > noise_freq - win) & (f < noise_freq + win)\n    # Surroundings (flanks)\n    surr_mask = ((f > noise_freq - 3) & (f < noise_freq - 1)) | (\n        (f > noise_freq + 1) & (f < noise_freq + 3)\n    )\n\n    ratio_orig = np.mean(psd_o[noise_mask]) / np.mean(psd_o[surr_mask])\n    ratio_clean = np.mean(psd_c[noise_mask]) / np.mean(psd_c[surr_mask])\n\n    return ratio_orig, ratio_clean\n\n\n# Overall\nratio_o, ratio_c = compute_noise_ratio(data, data_clean, sfreq, target_freq)\nprint(f\"Noise/Surroundings Ratio: {ratio_o:.2f}x \u2192 {ratio_c:.2f}x (target ~1.0)\")\n\n# Per segment\nratio_o1, ratio_c1 = compute_noise_ratio(\n    data[:, :t1], data_clean[:, :t1], sfreq, sim_info[\"freq1\"]\n)\nratio_o2, ratio_c2 = compute_noise_ratio(\n    data[:, t1:t2], data_clean[:, t1:t2], sfreq, sim_info[\"freq2\"]\n)\nratio_o3, ratio_c3 = compute_noise_ratio(\n    data[:, t2:], data_clean[:, t2:], sfreq, sim_info[\"freq3\"]\n)\n\nprint(f\"  Segment 1 (Strong):   {ratio_o1:.2f}x \u2192 {ratio_c1:.2f}x\")\nprint(f\"  Segment 2 (Moderate): {ratio_o2:.2f}x \u2192 {ratio_c2:.2f}x\")\nprint(f\"  Segment 3 (Weak):     {ratio_o3:.2f}x \u2192 {ratio_c3:.2f}x\")\n\n# Power removed\ntotal_power_orig = np.var(data)\ntotal_power_clean = np.var(data_clean)\npct_removed = (1 - total_power_clean / total_power_orig) * 100\nprint(f\"\\nTotal Power Removed: {pct_removed:.2f}%\")\n\nprint(\"\\n[OK] Example complete!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}